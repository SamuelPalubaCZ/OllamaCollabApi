{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelPalubaCZ/OllamaCollabApi/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk0QyUSuP-f2"
      },
      "source": [
        "<h2><hr>Do toho to listu zapište prosím modely které chcete stáhnout\n",
        "</h2> <h5>Pokud nemáš nejmenší tušení koukni se do komentáře nebo na obrázek </h5>\n",
        "<img title=\"List of models Ollama 2024\" src=\"https://miro.medium.com/v2/resize:fit:1362/1*36VW4LFnORnqR1mZ_Rrazw.png\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "85-xEtaCM8Kt"
      },
      "outputs": [],
      "source": [
        "#list_of_models = ['llama3.2','dolphin-llama3','UnknownFish/waltergpt','yantien/gemma2-uncensored','llama3.1:70b','monotykamary/medichat-llama3']\n",
        "#example\n",
        "list_of_models = ['MFDoom/deepseek-r1-tool-calling:70b']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMSvdvBLQymp"
      },
      "source": [
        "<br>\n",
        "<h2>**Metoda 1. ** Doporučeno</h2>\n",
        "<hr><h3>Skript od uživatele [SamuelPalubaCZ](https://github.com/SamuelPalubaCZ/)</hr></h3>\n",
        "\n",
        "\n",
        "*   Rozjede Ollamu s Veřejnou ip a api pomocí Cloudflared a najtes to potom na doméně [https://api.paluba.me](https://api.paluba.me)\n",
        "*   Nevyžaduje daší setup\n",
        "*   Spolehlivé\n",
        "*   Vhodné pro komerční využití"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKQfQQSiXiJA"
      },
      "source": [
        "<hr>\n",
        "<H3>Instalační Skript:</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iAl_LLX_GVrK",
        "outputId": "69fd7cb7-cd29-4fb4-b00c-53f76ffa1267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package lspci\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13269    0 13269    0     0  38473      0 --:--:-- --:--:-- --:--:-- 38572\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "--2025-01-29 12:48:04--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2025.1.0/cloudflared-linux-amd64.deb [following]\n",
            "--2025-01-29 12:48:04--  https://github.com/cloudflare/cloudflared/releases/download/2025.1.0/cloudflared-linux-amd64.deb\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/2d8414e3-a676-4d22-b0ad-71fffb4149a4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250129%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250129T124804Z&X-Amz-Expires=300&X-Amz-Signature=649740c3d009ab103327259a4869bdade87d74cd2168ec536a7e23305937fab4&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64.deb&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-01-29 12:48:05--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/2d8414e3-a676-4d22-b0ad-71fffb4149a4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250129%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250129T124804Z&X-Amz-Expires=300&X-Amz-Signature=649740c3d009ab103327259a4869bdade87d74cd2168ec536a7e23305937fab4&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64.deb&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18514648 (18M) [application/octet-stream]\n",
            "Saving to: ‘cloudflared-linux-amd64.deb.1’\n",
            "\n",
            "cloudflared-linux-a 100%[===================>]  17.66M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-01-29 12:48:06 (291 MB/s) - ‘cloudflared-linux-amd64.deb.1’ saved [18514648/18514648]\n",
            "\n",
            "(Reading database ... 124792 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
            "Unpacking cloudflared (2025.1.0) over (2025.1.0) ...\n",
            "Setting up cloudflared (2025.1.0) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y lspci libuv1 lshw pciutils cuda-drivers\n",
        "import os\n",
        "#check if ollama installed and if not install it\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import time\n",
        "import socket\n",
        "\n",
        "os.environ.update({'OLLAMA_HOST': '0.0.0.0'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "K1IupsyGMT1k"
      },
      "outputs": [],
      "source": [
        "def ollama(host='0.0.0.0:11434',origins='*',keepalive='-1'):\n",
        "    os.environ['OLLAMA_HOST'] = host\n",
        "    os.environ['OLLAMA_ORIGINS'] = origins\n",
        "    os.environ['OLLAMA_KEEP_ALIVE'] = keepalive\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "    print(\"Ollama server started\")\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "\n",
        "def install_models(lii=list_of_models):\n",
        "  for liu in lii:\n",
        "    !ollama pull {liu}\n",
        "  print(\"Models installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REE9LZXiRRPN"
      },
      "source": [
        "<hr>\n",
        "<h3>\n",
        "Iniciační skripta:\n",
        "</h3>\n",
        "      \n",
        "<h7>\n",
        "\n",
        "*   Třeba všechny postupně spustit\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<h5>\n",
        "\n",
        "\n",
        "\n",
        "1.   Pro start Ollamy\n",
        "2.   Pro start Cloudflared\n",
        "3.   Pro instalaci Modelu z listu\n",
        "\n",
        "</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "v7Vm8fkcRo2Z",
        "outputId": "3ede9858-9370-4536-d61b-d229cd9ef6d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama server started\n"
          ]
        }
      ],
      "source": [
        "ollama_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j1mmb4C6531v",
        "outputId": "54b70696-ba40-4a48-972a-4b32491c12ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-01-29T12:48:21Z\u001b[0m \u001b[32mINF\u001b[0m Using SysV\n",
            "\u001b[90m2025-01-29T12:48:21Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m error generating system template \u001b[31merror=\u001b[0m\u001b[31m\"cloudflared service is already installed at /etc/init.d/cloudflared; if you are running a cloudflared tunnel, you can point it to multiple origins, avoiding the need to run more than one cloudflared service in the same machine; otherwise if you are really sure, you can do `cloudflared service uninstall` to clean up the existing service and then try again this command\"\u001b[0m\n",
            "cloudflared service is already installed at /etc/init.d/cloudflared; if you are running a cloudflared tunnel, you can point it to multiple origins, avoiding the need to run more than one cloudflared service in the same machine; otherwise if you are really sure, you can do `cloudflared service uninstall` to clean up the existing service and then try again this command\n"
          ]
        }
      ],
      "source": [
        "!sudo cloudflared service install eyJhIjoiNzMxYjNmY2Q5ZTYyZTY2ZjcxYWUzNDI1NDU1NjhjYzciLCJ0IjoiYWE5ZDgwODQtZTk1My00OGJiLWIzYjUtOTc5ZTlhN2U1MDQwIiwicyI6Ik1EQXhOR0V4TW1FdE5UVTJOUzAwWVRFMkxUZzNaVFV0TTJVNU9EUmpZV0pqWkdNNCJ9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "v4tKZFd1XGKy",
        "outputId": "22cd2487-29d8-4ea3-cc13-da6c387c4864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 4cd576d9aa16... 100% ▕▏  42 GB                         \n",
            "pulling 6e4c38e1172f... 100% ▕▏ 1.1 KB                         \n",
            "pulling d86a077a82d0... 100% ▕▏ 1.7 KB                         \n",
            "pulling ef78cf68b02a... 100% ▕▏  166 B                         \n",
            "pulling fcf6a150fd44... 100% ▕▏  488 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n",
            "Models installed\n"
          ]
        }
      ],
      "source": [
        "install_models()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNgGejdXUMcq7HUjKzWTDHm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}